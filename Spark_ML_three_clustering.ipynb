{"cells":[{"cell_type":"code","source":["df = spark.createDataFrame([[0, 33.3, -17.5],\n                              [1, 40.4, -20.5],\n                              [2, 28., -23.9],\n                              [3, 29.5, -19.0],\n                              [4, 32.8, -18.84]\n                             ],\n                              [\"id\",\"lat\", \"long\"])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+------+\n id| lat|  long|\n+---+----+------+\n  0|33.3| -17.5|\n  1|40.4| -20.5|\n  2|28.0| -23.9|\n  3|29.5| -19.0|\n  4|32.8|-18.84|\n+---+----+------+\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nvecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df)\nnew_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+------+-------------+\n id| lat|  long|     features|\n+---+----+------+-------------+\n  0|33.3| -17.5| [33.3,-17.5]|\n  1|40.4| -20.5| [40.4,-20.5]|\n  2|28.0| -23.9| [28.0,-23.9]|\n  3|29.5| -19.0| [29.5,-19.0]|\n  4|32.8|-18.84|[32.8,-18.84]|\n+---+----+------+-------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=3, seed=1)  # 3 clusters here\nmodel = kmeans.fit(new_df.select('features'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["transformed = model.transform(new_df)\ntransformed.show() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+------+-------------+----------+\n id| lat|  long|     features|prediction|\n+---+----+------+-------------+----------+\n  0|33.3| -17.5| [33.3,-17.5]|         0|\n  1|40.4| -20.5| [40.4,-20.5]|         1|\n  2|28.0| -23.9| [28.0,-23.9]|         2|\n  3|29.5| -19.0| [29.5,-19.0]|         0|\n  4|32.8|-18.84|[32.8,-18.84]|         0|\n+---+----+------+-------------+----------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Evaluate clustering by computing Within Set Sum of Squared Errors.\nwssse = model.computeCost(new_df)\nprint(\"Within Set Sum of Squared Errors = \" + str(wssse))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Within Set Sum of Squared Errors = 9.883733333332202\nCluster Centers: \n[ 31.86666667 -18.44666667]\n[ 40.4 -20.5]\n[ 28.  -23.9]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["#type 转换 for HW1 \ndata_list = [{'name': 'Alice', 'age': '1'}, \n     {'name': 'Mike', 'age': '2'},\n     {'name': 'json', 'age': '3'},\n    ]\n \ndf_new = spark.createDataFrame(data_list)\n\nfrom pyspark.sql.types import DoubleType\n\nchangedTypedf = df_new.withColumn(\"age\", df_new[\"age\"].cast(DoubleType()))\n\nchangedTypedf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/session.py:355: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n  warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n+---+-----+\nage| name|\n+---+-----+\n1.0|Alice|\n2.0| Mike|\n3.0| json|\n+---+-----+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"Spark_ML_three_clustering","notebookId":235873308282536},"nbformat":4,"nbformat_minor":0}
